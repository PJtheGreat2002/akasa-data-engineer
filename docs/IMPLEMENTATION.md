# ğŸ’» Akasa Air Analytics - Implementation Guide

Detailed implementation guide covering code structure, key modules, and development best practices.

---

## ğŸ“‹ Table of Contents

1. [Project Structure](#project-structure)
2. [Core Modules](#core-modules)
3. [Data Pipeline Implementation](#data-pipeline-implementation)
4. [KPI Engine Implementation](#kpi-engine-implementation)
5. [UI Implementation](#ui-implementation)
6. [Testing Strategy](#testing-strategy)
7. [Code Standards](#code-standards)
8. [Common Patterns](#common-patterns)
9. [Extending the Application](#extending-the-application)

---

## Project Structure

```
akasa-data-pipeline/
â”‚
â”œâ”€â”€ app.py                          # Main Streamlit entry point
â”œâ”€â”€ docker-compose.yml              # Docker configuration
â”œâ”€â”€ init.sql                        # Database schema
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ .env                           # Environment variables (not in git)
â”œâ”€â”€ .env.example                   # Environment template
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”‚
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml                # Streamlit configuration
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Application configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Input CSV/XML files
â”‚   â”œâ”€â”€ processed/                 # Processed data cache
â”‚   â””â”€â”€ samples/                   # Sample data
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                 # Data input layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ csv_loader.py         # CSV processing
â”‚   â”‚   â”œâ”€â”€ xml_loader.py         # XML processing
â”‚   â”‚
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                  # Data access
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ db_manager.py         # Connection management
â”‚   â”‚
â”‚   â”œâ”€â”€ kpis/                      # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_kpi.py           # Base KPI class
â”‚   â”‚   â”œâ”€â”€ table_based_kpis.py   # SQL KPIs
â”‚   â”‚   â””â”€â”€ memory_based_kpis.py  # Pandas KPIs
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py             # Logging
â”‚       â”œâ”€â”€ validators.py         # Validation utils
â”‚
â”œâ”€â”€ pages/                         # Streamlit pages
â”‚   â”œâ”€â”€ 1_Overview.py
â”‚   â”œâ”€â”€ 2_KPI_Analytics.py
â”‚   â”œâ”€â”€ 3_Data_Management.py
â”‚   â””â”€â”€ 4_System_Logs.py
â”‚
â”œâ”€â”€ logs/                          # Application logs
â”‚   â””â”€â”€ app_YYYYMMDD.log
â”‚
â””â”€â”€ docs/                          # Documentation
    â”œâ”€â”€ SETUP.md
    â”œâ”€â”€ DESIGN.md
    â”œâ”€â”€ IMPLEMENTATION.md (this file)
    â””â”€â”€ QUICKSTART.md
```

---

## Core Modules

### 1. Configuration Module

**File**: `config/config.py`

```python
"""
Centralized configuration management
Key Features:
- Environment variable loading
- Path management
- Feature flags
"""

class Config:
    # Paths
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / 'data'
    
    # Database (from db_config)
    DB_CONFIG = DatabaseConfig.get_connection_config()
    
    # Application settings
    LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
    
    @classmethod
    def ensure_directories(cls):
        """Create directories if they don't exist"""
        cls.DATA_DIR.mkdir(exist_ok=True)
```

**Usage**:
```python
from config.config import Config

# Access configuration
data_path = Config.RAW_DATA_DIR
log_level = Config.LOG_LEVEL
```

---

### 2. Database Manager

**File**: `src/database/db_manager.py`

```python
"""
Database connection pooling and management
Key Features:
- Singleton pattern
- Connection pooling
- Context managers for safe operations
- Both MySQL Connector and SQLAlchemy support
"""

class DatabaseManager:
    _instance = None  # Singleton
    
    def __init__(self):
        self._initialize_connection_pool()
        self._initialize_sqlalchemy_engine()
    
    @contextmanager
    def get_cursor(self, dictionary=True):
        """
        Safe cursor management
        Automatically handles:
        - Connection acquisition
        - Transaction commit/rollback
        - Resource cleanup
        """
        connection = None
        cursor = None
        try:
            connection = self.get_connection()
            cursor = connection.cursor(dictionary=dictionary)
            yield cursor
            connection.commit()
        except Error as e:
            if connection:
                connection.rollback()
            raise
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()
```

**Usage**:
```python
from src.database.db_manager import db_manager

# Execute query safely
with db_manager.get_cursor() as cursor:
    cursor.execute("SELECT * FROM customers")
    results = cursor.fetchall()

# Or use convenience method
results = db_manager.execute_query(
    "SELECT * FROM customers WHERE region = %s",
    ("North",)
)
```

**Key Concepts**:
- **Connection Pooling**: Reuses connections for efficiency
- **Context Managers**: Automatic resource cleanup
- **Parameterized Queries**: SQL injection prevention

---

### 3. Logger Module

**File**: `src/utils/logger.py`

```python
"""
Centralized logging with color-coded output
Key Features:
- Colored console output
- File logging with rotation
- Different log levels
"""

def setup_logger(name: str = __name__, log_file: str = None):
    """
    Creates logger with:
    - Console handler (colored)
    - File handler (rotating)
    - Proper formatting
    """
    logger = logging.getLogger(name)
    
    # Console handler with colors
    console_handler = colorlog.StreamHandler()
    console_format = colorlog.ColoredFormatter(...)
    
    # File handler
    file_handler = logging.FileHandler(log_path)
    file_format = logging.Formatter(...)
    
    return logger
```

**Usage**:
```python
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

logger.info("Processing started")
logger.warning("Invalid data found")
logger.error("Database connection failed")
```

**Log Levels**:
- **DEBUG**: Detailed information for diagnostics
- **INFO**: General information messages
- **WARNING**: Warning messages
- **ERROR**: Error messages
- **CRITICAL**: Critical issues

---

## Data Pipeline Implementation

### CSV Loader

**File**: `src/ingestion/csv_loader.py`

```python
class CSVLoader:
    """
    CSV file processing pipeline:
    1. Load CSV file
    2. Validate structure and data
    3. Clean and normalize
    4. Load to database
    """
    
    def process_csv(self, file_path, mode='replace'):
        """
        Complete processing pipeline
        
        Args:
            file_path: Path to CSV file
            mode: 'replace' (clear existing) or 'append' (add to existing)
            
        Returns:
            Dictionary with results and metrics
        """
        # 1. Load
        df = self.load_csv(file_path)
        
        # 2. Validate
        is_valid, errors = self.validate_dataframe(df)
        if not is_valid:
            return {'success': False, 'errors': errors}
        
        # 3. Clean
        df_clean = self.clean_dataframe(df)
        
        # 4. Load to DB
        success = self.load_to_database(df_clean, mode)
        
        return {
            'success': success,
            'records_loaded': len(df_clean),
            'duration': time_taken
        }
```

**Key Methods**:

```python
def load_csv(self, file_path):
    """Load CSV into Pandas DataFrame"""
    df = pd.read_csv(file_path, dtype={'customer_id': str})
    return df

def validate_dataframe(self, df):
    """
    Validate:
    - Required columns exist
    - Data types are correct
    - Business rules are met
    """
    errors = []
    # Check columns
    # Validate each row
    return is_valid, errors

def clean_dataframe(self, df):
    """
    Clean:
    - Remove whitespace
    - Normalize mobile numbers
    - Remove duplicates
    - Handle missing values
    """
    df_clean = df.copy()
    # Cleaning steps
    return df_clean

def load_to_database(self, df, mode):
    """
    Load to MySQL:
    - Clear existing data if mode='replace'
    - Batch insert using executemany
    - Handle errors gracefully
    """
    if mode == 'replace':
        db_manager.execute_query("DELETE FROM customers", fetch=False)
    
    data = [(row['customer_id'], row['customer_name'], ...) for _, row in df.iterrows()]
    db_manager.execute_many(insert_query, data)
```

---

### XML Loader

**File**: `src/ingestion/xml_loader.py`

```python
class XMLLoader:
    """
    XML file processing pipeline
    Similar to CSV but handles XML structure
    """
    
    def load_xml(self, file_path):
        """Parse XML file into list of dictionaries"""
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        orders = []
        for order_elem in root.findall('order'):
            order = {
                'order_id': order_elem.find('order_id').text,
                'mobile_number': order_elem.find('mobile_number').text,
                # ... other fields
            }
            orders.append(order)
        
        return orders
```

**XML Structure Expected**:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<orders>
    <order>
        <order_id>O001</order_id>
        <mobile_number>+919123456781</mobile_number>
        <order_date_time>2025-10-15T14:30:00</order_date_time>
        <sku_id>SKU123</sku_id>
        <sku_count>2</sku_count>
        <total_amount>1500.00</total_amount>
    </order>
    <!-- More orders -->
</orders>
```

---

### Data Validation

**File**: `src/transformation/data_validator.py`

```python
class DataValidator:
    """
    Comprehensive data validation
    - Schema validation
    - Data type validation
    - Business rule validation
    - Referential integrity checks
    """
    
    def validate_customer_data(self, df):
        """
        Validate customer DataFrame
        
        Checks:
        - Required columns exist
        - customer_id is valid (1-25 chars)
        - customer_name is valid (min 2 chars)
        - mobile_number is valid (8-15 digits)
        - region is valid (min 2 chars)
        """
        errors = []
        
        for idx, row in df.iterrows():
            # Validate each field
            if not self._is_valid_customer_id(row['customer_id']):
                errors.append(f"Row {idx}: Invalid customer_id")
            # ... more validations
        
        return len(errors) == 0, errors
    
    def validate_business_rules(self, orders_df):
        """
        Business-specific validations
        - Order amount reasonable (>0, <1M)
        - SKU count reasonable (1-100)
        - Order date not in future
        """
        errors = []
        
        # Check amounts
        invalid = orders_df[
            (orders_df['total_amount'] <= 0) | 
            (orders_df['total_amount'] > 1000000)
        ]
        if not invalid.empty:
            errors.append(f"Found {len(invalid)} invalid amounts")
        
        return len(errors) == 0, errors
```

---

## KPI Engine Implementation

### Base KPI Class

**File**: `src/kpis/base_kpi.py`

```python
class BaseKPI(ABC):
    """
    Abstract base class for all KPIs
    Implements Template Method pattern
    """
    
    def __init__(self, name, description):
        self.name = name
        self.description = description
        self.last_result = None
    
    @abstractmethod
    def calculate(self, **kwargs):
        """Subclasses must implement this"""
        pass
    
    def _format_result(self, data, metadata=None):
        """Standard result format"""
        return {
            'kpi_name': self.name,
            'description': self.description,
            'data': data,
            'calculated_at': datetime.now().isoformat(),
            'success': True,
            'metadata': metadata or {}
        }
```

---

### Table-Based KPIs (SQL)

**File**: `src/kpis/table_based_kpis.py`

```python
class RepeatCustomersKPI(BaseKPI):
    """KPI 1: Customers with more than one order"""
    
    def calculate(self, **kwargs):
        """Calculate using SQL query"""
        query = """
            SELECT 
                c.customer_id,
                c.customer_name,
                COUNT(o.order_id) as order_count,
                SUM(o.total_amount) as total_spent
            FROM customers c
            INNER JOIN orders o ON c.mobile_number = o.mobile_number
            GROUP BY c.customer_id, c.customer_name
            HAVING COUNT(o.order_id) > 1
            ORDER BY order_count DESC
        """
        
        results = db_manager.execute_query(query)
        
        metadata = {
            'total_repeat_customers': len(results),
            'total_orders': sum(r['order_count'] for r in results)
        }
        
        return self._format_result(results, metadata)
```

**Why SQL approach?**
- âœ… Efficient for large datasets (database does the work)
- âœ… Leverages database indexes
- âœ… Less memory usage (no data transfer to Python)
- âŒ Limited by SQL capabilities

---

### Memory-Based KPIs (Pandas)

**File**: `src/kpis/memory_based_kpis.py`

```python
class MemoryBasedKPIEngine:
    """In-memory KPI calculations using Pandas"""
    
    def __init__(self):
        self.customers_df = None
        self.orders_df = None
    
    def load_data(self):
        """Load all data into DataFrames"""
        customers_data = db_manager.execute_query("SELECT * FROM customers")
        self.customers_df = pd.DataFrame(customers_data)
        
        orders_data = db_manager.execute_query("SELECT * FROM orders")
        self.orders_df = pd.DataFrame(orders_data)
        
        # Convert data types
        self.orders_df['order_date_time'] = pd.to_datetime(
            self.orders_df['order_date_time']
        )
    
    def calculate_repeat_customers(self):
        """Calculate using Pandas operations"""
        # Merge dataframes
        merged = self.customers_df.merge(
            self.orders_df,
            on='mobile_number',
            how='inner'
        )
        
        # Group and aggregate
        customer_orders = merged.groupby(
            ['customer_id', 'customer_name']
        ).agg({
            'order_id': 'count',
            'total_amount': 'sum'
        }).reset_index()
        
        # Rename columns
        customer_orders.columns = [
            'customer_id', 'customer_name', 
            'order_count', 'total_spent'
        ]
        
        # Filter repeat customers
        repeat_customers = customer_orders[
            customer_orders['order_count'] > 1
        ]
        
        # Sort
        repeat_customers = repeat_customers.sort_values(
            by=['order_count', 'total_spent'],
            ascending=[False, False]
        )
        
        return repeat_customers.to_dict('records')
```

**Why Pandas approach?**
- âœ… More flexible transformations
- âœ… Better for complex calculations
- âœ… Easier to debug
- âŒ Memory intensive for large datasets
- âŒ Slower for simple aggregations

---

## UI Implementation

### Main Application

**File**: `app.py`

```python
import streamlit as st
from config.config import Config

# Page configuration
st.set_page_config(
    page_title=Config.PAGE_TITLE,
    page_icon=Config.PAGE_ICON,
    layout="wide"
)

def main():
    # Header
    st.title(f"{Config.PAGE_ICON} {Config.PAGE_TITLE}")
    
    # Sidebar
    with st.sidebar:
        st.markdown("### Navigation")
        # Sidebar content
    
    # Main content
    st.markdown("## Welcome")
    
    # Quick stats
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Customers", total_customers)
    with col2:
        st.metric("Orders", total_orders)
    with col3:
        st.metric("Revenue", f"â‚¹{total_revenue:,.2f}")

if __name__ == "__main__":
    main()
```

---

### Page Structure

**File**: `pages/1_ğŸ“Š_Overview.py`

```python
import streamlit as st
from src.kpis.table_based_kpis import table_kpi_engine

st.title("ğŸ“Š Overview Dashboard")

# Caching for performance
@st.cache_data(ttl=Config.CACHE_TTL)
def load_overview_data():
    # Load data logic
    return data

# Load data
data = load_overview_data()

# Display metrics
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Metric 1", data['value1'])

# Display charts
fig = create_chart(data)
st.plotly_chart(fig, use_container_width=True)
```

**Streamlit Key Concepts**:

1. **Caching**:
```python
@st.cache_data(ttl=600)  # Cache for 10 minutes
def expensive_function():
    return results
```

2. **Layout**:
```python
# Columns
col1, col2 = st.columns(2)
with col1:
    st.write("Left column")

# Tabs
tab1, tab2 = st.tabs(["Tab 1", "Tab 2"])
with tab1:
    st.write("Tab 1 content")

# Expander
with st.expander("Show details"):
    st.write("Hidden content")
```

3. **Widgets**:
```python
# Input
text = st.text_input("Enter text")
number = st.number_input("Enter number")
date = st.date_input("Select date")

# Selection
option = st.selectbox("Choose", options)
options = st.multiselect("Choose multiple", choices)

# Action
if st.button("Click me"):
    st.write("Button clicked!")
```

---

### Charts Implementation

**Using Plotly**:

```python
import plotly.express as px
import plotly.graph_objects as go

# Line chart
fig = px.line(
    df,
    x='month',
    y='revenue',
    title='Monthly Revenue',
    markers=True
)
st.plotly_chart(fig, use_container_width=True)

# Bar chart
fig = px.bar(
    df,
    x='region',
    y='total_revenue',
    color='total_revenue',
    color_continuous_scale='Viridis'
)
st.plotly_chart(fig)

# Pie chart
fig = px.pie(
    df,
    values='revenue',
    names='region',
    hole=0.4  # Donut chart
)
st.plotly_chart(fig)

# Advanced: Multiple traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=x, y=y1, name='Line 1'))
fig.add_trace(go.Scatter(x=x, y=y2, name='Line 2'))
fig.update_layout(title='Multiple Lines')
st.plotly_chart(fig)
```

---

## Testing Strategy

### Unit Tests

**File**: `tests/test_loaders.py`

```python
import pytest
from src.ingestion.csv_loader import csv_loader

def test_csv_loading():
    """Test CSV file loading"""
    result = csv_loader.process_csv()
    
    assert result['success'] == True
    assert result['records_loaded'] > 0
    assert 'errors' in result

def test_csv_validation():
    """Test CSV validation"""
    # Create test DataFrame
    df = pd.DataFrame({
        'customer_id': ['C001', 'C002'],
        'customer_name': ['Test', 'User'],
        'mobile_number': ['9123456781', '9123456782'],
        'region': ['North', 'South']
    })
    
    is_valid, errors = csv_loader.validate_dataframe(df)
    assert is_valid == True
    assert len(errors) == 0
```

### Integration Tests

```python
def test_end_to_end_pipeline():
    """Test complete data pipeline"""
    # 1. Load CSV
    csv_result = csv_loader.process_csv()
    assert csv_result['success']
    
    # 2. Load XML
    xml_result = xml_loader.process_xml()
    assert xml_result['success']
    
    # 3. Calculate KPI
    kpi_result = table_kpi_engine.calculate_kpi('repeat_customers')
    assert kpi_result['success']
    assert len(kpi_result['data']) > 0
```

### Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_loaders.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html
```

---

## Code Standards

### Python Style Guide

Following **PEP 8** standards:

```python
# Good: Clear naming
def calculate_customer_lifetime_value(customer_id):
    pass

# Bad: Unclear naming
def calc_clv(cid):
    pass

# Good: Docstrings
def process_data(data: pd.DataFrame) -> pd.DataFrame:
    """
    Process customer data
    
    Args:
        data: Input DataFrame
        
    Returns:
        Processed DataFrame
    """
    pass

# Good: Type hints
def get_customer(customer_id: str) -> Optional[Dict[str, Any]]:
    pass

# Good: Constants
MAX_RETRY_ATTEMPTS = 3
DEFAULT_TIMEOUT = 30

# Good: Error handling
try:
    result = risky_operation()
except SpecificError as e:
    logger.error(f"Error: {e}")
    raise
finally:
    cleanup()
```

### File Organization

```python
"""
Module docstring: Brief description of the module
"""

# Standard library imports
import os
import sys
from datetime import datetime

# Third-party imports
import pandas as pd
import numpy as np

# Local imports
from config.config import Config
from src.utils.logger import setup_logger

# Constants
MAX_RETRIES = 3
DEFAULT_TIMEOUT = 30

# Module-level logger
logger = setup_logger(__name__)

# Classes and functions
class MyClass:
    pass

def my_function():
    pass

# Singleton instance (if applicable)
instance = MyClass()
```

---

## Common Patterns

### 1. Context Manager Pattern

```python
# Database operations
with db_manager.get_cursor() as cursor:
    cursor.execute(query)
    results = cursor.fetchall()
# Auto-cleanup handled

# File operations
with open(file_path, 'r') as f:
    content = f.read()
# File auto-closed
```

### 2. Decorator Pattern

```python
# Caching
@st.cache_data(ttl=600)
def expensive_operation():
    return result

# Logging
def log_execution_time(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start
        logger.info(f"{func.__name__} took {duration:.2f}s")
        return result
    return wrapper

@log_execution_time
def slow_function():
    pass
```

### 3. Error Handling Pattern

```python
def safe_operation():
    """
    Comprehensive error handling
    """
    try:
        # Attempt operation
        result = risky_operation()
        return {'success': True, 'data': result}
        
    except SpecificError as e:
        # Handle specific error
        logger.error(f"Specific error: {e}")
        return {'success': False, 'error': str(e)}
        
    except Exception as e:
        # Handle unexpected errors
        logger.error(f"Unexpected error: {e}", exc_info=True)
        return {'success': False, 'error': 'Unexpected error occurred'}
        
    finally:
        # Always execute cleanup
        cleanup_resources()
```

---

## Extending the Application

### Adding a New KPI

**Step 1**: Create KPI class

```python
# src/kpis/table_based_kpis.py

class NewKPI(BaseKPI):
    def __init__(self):
        super().__init__(
            name="My New KPI",
            description="Description of what it calculates"
        )
    
    def calculate(self, **kwargs):
        query = """
            SELECT ... 
            FROM ... 
            WHERE ...
        """
        results = db_manager.execute_query(query)
        return self._format_result(results)
```

**Step 2**: Register in engine

```python
# src/kpis/table_based_kpis.py

class TableBasedKPIEngine:
    def __init__(self):
        self.kpis = {
            'repeat_customers': RepeatCustomersKPI(),
            'monthly_trends': MonthlyTrendsKPI(),
            'new_kpi': NewKPI(),  # Add here
        }
```

**Step 3**: Add to UI

```python
# pages/2_ğŸ“ˆ_KPI_Analytics.py

kpi_options = [
    "Repeat Customers",
    "Monthly Trends",
    "My New KPI",  # Add here
]
```

---

### Adding a New Page

**Step 1**: Create page file

```python
# pages/5_ğŸ†•_New_Page.py

import streamlit as st

st.set_page_config(
    page_title="New Page",
    page_icon="ğŸ†•",
    layout="wide"
)

st.title("ğŸ†• New Page")
st.write("Content here")
```

**Step 2**: Streamlit auto-detects it!

---

### Adding a New Data Source

**Step 1**: Create loader

```python
# src/ingestion/json_loader.py

class JSONLoader:
    def load_json(self, file_path):
        with open(file_path, 'r') as f:
            data = json.load(f)
        return data
    
    def validate_json(self, data):
        # Validation logic
        pass
    
    def process_json(self, file_path):
        # Complete pipeline
        pass
```

**Step 2**: Add to data management page

```python
# pages/3_âš™ï¸_Data_Management.py

json_file = st.file_uploader("Upload JSON", type=['json'])
if json_file:
    result = json_loader.process_json(json_file)
```

---

## Best Practices Checklist

### Code Quality
- [ ] Follow PEP 8 style guide
- [ ] Add docstrings to all functions/classes
- [ ] Use type hints
- [ ] Handle errors gracefully
- [ ] Log important events
- [ ] Write unit tests

### Security
- [ ] Use parameterized queries
- [ ] Store credentials in .env
- [ ] Validate all user inputs
- [ ] Sanitize error messages
- [ ] Use HTTPS in production

### Performance
- [ ] Use caching appropriately
- [ ] Optimize database queries
- [ ] Use connection pooling
- [ ] Load data lazily
- [ ] Profile slow operations

### Maintainability
- [ ] Keep functions small and focused
- [ ] Avoid code duplication
- [ ] Use meaningful names
- [ ] Comment complex logic
- [ ] Keep dependencies updated

---

## Useful Commands

### Development

```bash
# Start development server
streamlit run app.py

# Run with hot reload
streamlit run app.py --server.runOnSave true

### Database

```bash
# Connect to MySQL
docker exec -it akasa_mysql mysql -u<username> -p<password> <database_name>

# View logs
docker logs akasa_mysql
```

---

## Debugging Tips

### 1. Streamlit Debugging

```python
# Print to console (shows in terminal)
print(f"Debug: {variable}")

# Display in UI
st.write("Debug:", variable)

# Show DataFrame
st.dataframe(df)

# Show as JSON
st.json(data)

# Show exception
try:
    operation()
except Exception as e:
    st.exception(e)
```

### 2. Database Debugging

```python
# Enable SQL query logging
# In db_manager.py
engine = create_engine(url, echo=True)  # Shows all SQL queries

# Test query directly
result = db_manager.execute_query("SELECT * FROM customers LIMIT 5")
print(result)
```

### 3. Performance Debugging

```python
import time

start = time.time()
expensive_operation()
duration = time.time() - start
logger.info(f"Operation took {duration:.2f}s")

# Use st.spinner for visual feedback
with st.spinner("Processing..."):
    result = expensive_operation()
```

---

## Conclusion

This implementation guide covers:

âœ… **Project structure and organization**  
âœ… **Core module implementations**  
âœ… **Data pipeline architecture**  
âœ… **KPI calculation engines**  
âœ… **UI implementation patterns**  
âœ… **Testing strategies**  
âœ… **Code standards and best practices**  
âœ… **Extension guidelines**  

For setup instructions, see `SETUP.md`  
For architecture details, see `DESIGN.md`

---

**Happy coding! ğŸš€**